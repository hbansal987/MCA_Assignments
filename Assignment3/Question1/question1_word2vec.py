# -*- coding: utf-8 -*-
"""Question1_Word2Vec

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Wd5qIYXMyBWeRC_rvG1FGvU0WrpqfGm
"""

import nltk
nltk.download('abc')
import nltk
from nltk.corpus import abc
word_list = (nltk.corpus.abc.words())
sentence_list = nltk.corpus.abc.sents()

import numpy as np
###Change the vocab size my removing the stopwords 
vocab_size = len(word_list)
window_size = 5
vector_dim = 300

"""For similarity checking"""
###Change a bit
valid_size = 16
valid_window = 100
valid_examples = np.random.choice(valid_window, valid_size, replace=False)

def tokenizer(corpus):
  """Corpus already is changed to tokens we just need to change them to lower"""
  final_list = []
  for i in corpus:
    ###Need to remove the stopwords
    final_list.append(i.lower())
  return(final_list)

def id_mapping(token):
  word_to_id_mapping = {}
  id_to_word_mapping = {}
  ###I think mapping is a bit faulty cause the id changes when the word is repeated
  i=0;
  for text in token:
    #if(i==0):
      #print(text)
    word_to_id_mapping[text] = i
    id_to_word_mapping[i] = text
    i=i+1

  return word_to_id_mapping,id_to_word_mapping

token = tokenizer(word_list)
word_to_id_mapping , id_to_word_mapping = id_mapping(token)

data = []
for word in word_to_id_mapping:
  data.append(word_to_id_mapping[word])

from keras.models import Model
from keras.layers import Input, Dense, Reshape, merge
from keras.layers.embeddings import Embedding
from keras.preprocessing.sequence import skipgrams
from keras.preprocessing import sequence
import tensorflow as tf

sample_matrix = sequence.make_sampling_table(vocab_size) 
couple,label = skipgrams(data, vocab_size, window_size=window_size, sampling_table =sample_matrix)

"""Couple is the skipgram 
Label is the one hot encoding"""

"""Embedding"""

target_input = Input((1,))
context_input = Input((1,))

embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')
target = embedding(target_input)
context = embedding(context_input)

target = Reshape((vector_dim,1))(target) #Reshaping according to vector dimension size
context = Reshape((vector_dim,1))(context)

"""Similarity Function"""
from keras.layers import Input, Dense, Reshape, merge, Dot


similar = Dot(axes=1,normalize = True)([target, context])
dot_product = Dot(axes=1, normalize = False)([target, context])
dot_product = Reshape((1,))(dot_product)

output = Dense(1, activation='sigmoid')(dot_product)

model = Model(input=[target_input, context_input], output=output)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

validation_model = Model(input=[target_input, context_input], output=similar)

## Can be chaanged
word_target, word_context = zip(*couple) 
word_target = np.array(word_target)
word_context = np.array(word_context)

import glob
import pickle
import matplotlib.pyplot as plt
import matplotlib.cm as cm

for cnt in range(200):
  loss = model.train_on_batch([word_target, word_context], label)
  print("Iteration {}, loss={}".format(cnt, loss))
  
output1 = open('/content/drive/My Drive/Assignment3/model_full'+ '.pickle', 'wb')
pickle.dump(model, output1)