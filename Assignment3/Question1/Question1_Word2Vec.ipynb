{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1_Word2Vec",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LND5X9gKJ1dB",
        "colab_type": "code",
        "outputId": "2b1a65ac-8e54-4f65-9b05-ec6b76d92559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('abc')\n",
        "import nltk\n",
        "from nltk.corpus import abc\n",
        "word_list = (nltk.corpus.abc.words())\n",
        "sentence_list = nltk.corpus.abc.sents()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jh_gdr-KJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "###Change the vocab size my removing the stopwords \n",
        "vocab_size = len(word_list)\n",
        "window_size = 5\n",
        "vector_dim = 300\n",
        "\n",
        "\"\"\"For similarity checking\"\"\"\n",
        "###Change a bit\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJLHSRJHyd1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(corpus):\n",
        "  \"\"\"Corpus already is changed to tokens we just need to change them to lower\"\"\"\n",
        "  final_list = []\n",
        "  for i in corpus:\n",
        "    ###Need to remove the stopwords\n",
        "    final_list.append(i.lower())\n",
        "  return(final_list)\n",
        "\n",
        "def id_mapping(token):\n",
        "  word_to_id_mapping = {}\n",
        "  id_to_word_mapping = {}\n",
        "  ###I think mapping is a bit faulty cause the id changes when the word is repeated\n",
        "  i=0;\n",
        "  for text in token:\n",
        "    #if(i==0):\n",
        "      #print(text)\n",
        "    word_to_id_mapping[text] = i\n",
        "    id_to_word_mapping[i] = text\n",
        "    i=i+1\n",
        "\n",
        "  return word_to_id_mapping,id_to_word_mapping\n",
        "\n",
        "token = tokenizer(word_list)\n",
        "word_to_id_mapping , id_to_word_mapping = id_mapping(token)\n",
        "\n",
        "data = []\n",
        "for word in word_to_id_mapping:\n",
        "  data.append(word_to_id_mapping[word])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDNJMhXJ1vEj",
        "colab_type": "code",
        "outputId": "768a12d9-a6c5-47be-d4fe-eb971a404d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Reshape, merge\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "\n",
        "sample_matrix = sequence.make_sampling_table(vocab_size) \n",
        "couple,label = skipgrams(data, vocab_size, window_size=window_size, sampling_table =sample_matrix)\n",
        "\n",
        "\"\"\"Couple is the skipgram \n",
        "Label is the one hot encoding\"\"\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Couple is the skipgram \\nLabel is the one hot encoding'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfVeU_zC4iw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Embedding\"\"\"\n",
        "\n",
        "target_input = Input((1,))\n",
        "context_input = Input((1,))\n",
        "\n",
        "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
        "target = embedding(target_input)\n",
        "context = embedding(context_input)\n",
        "\n",
        "target = Reshape((vector_dim,1))(target) #Reshaping according to vector dimension size\n",
        "context = Reshape((vector_dim,1))(context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbCmMDdi9s6C",
        "colab_type": "code",
        "outputId": "d60dc0be-5797-413b-f84d-387cf3a008d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\"\"\"Similarity Function\"\"\"\n",
        "from keras.layers import Input, Dense, Reshape, merge, Dot\n",
        "\n",
        "\n",
        "similar = Dot(axes=1,normalize = True)([target, context])\n",
        "dot_product = Dot(axes=1, normalize = False)([target, context])\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "output = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "model = Model(input=[target_input, context_input], output=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mVqibGj-UnW",
        "colab_type": "code",
        "outputId": "602561a2-80fd-48b4-c6e8-c7b7ceb69f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "validation_model = Model(input=[target_input, context_input], output=similar)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lin7jN1qjQ5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Can be chaanged\n",
        "word_target, word_context = zip(*couple) \n",
        "word_target = np.array(word_target)\n",
        "word_context = np.array(word_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFkH7hOSirnB",
        "colab_type": "code",
        "outputId": "5f51b1b5-6e1c-4158-bc9b-84e3d739af26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "for cnt in range(200):\n",
        "  loss = model.train_on_batch([word_target, word_context], label)\n",
        "  print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
        "  \n",
        "output1 = open('/content/drive/My Drive/Assignment3/model_full'+ '.pickle', 'wb')\n",
        "pickle.dump(model, output1)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 0, loss=[0.69317275, 0.49945477]\n",
            "Iteration 1, loss=[0.69308156, 0.5046868]\n",
            "Iteration 2, loss=[0.69295585, 0.5119516]\n",
            "Iteration 3, loss=[0.6928043, 0.5206319]\n",
            "Iteration 4, loss=[0.69263107, 0.5307095]\n",
            "Iteration 5, loss=[0.69243884, 0.54145515]\n",
            "Iteration 6, loss=[0.69222945, 0.5535873]\n",
            "Iteration 7, loss=[0.692004, 0.5664867]\n",
            "Iteration 8, loss=[0.6917635, 0.5799657]\n",
            "Iteration 9, loss=[0.6915086, 0.5942643]\n",
            "Iteration 10, loss=[0.6912396, 0.6092959]\n",
            "Iteration 11, loss=[0.6909571, 0.6248547]\n",
            "Iteration 12, loss=[0.6906613, 0.6408016]\n",
            "Iteration 13, loss=[0.6903524, 0.6570681]\n",
            "Iteration 14, loss=[0.69003034, 0.6736234]\n",
            "Iteration 15, loss=[0.68969536, 0.69029427]\n",
            "Iteration 16, loss=[0.6893473, 0.7072053]\n",
            "Iteration 17, loss=[0.6889862, 0.7241596]\n",
            "Iteration 18, loss=[0.68861187, 0.74139553]\n",
            "Iteration 19, loss=[0.6882242, 0.75844556]\n",
            "Iteration 20, loss=[0.68782294, 0.7756183]\n",
            "Iteration 21, loss=[0.6874079, 0.7922116]\n",
            "Iteration 22, loss=[0.6869789, 0.8086279]\n",
            "Iteration 23, loss=[0.6865355, 0.8245911]\n",
            "Iteration 24, loss=[0.68607736, 0.83977973]\n",
            "Iteration 25, loss=[0.68560433, 0.85473734]\n",
            "Iteration 26, loss=[0.68511575, 0.869063]\n",
            "Iteration 27, loss=[0.6846114, 0.8826756]\n",
            "Iteration 28, loss=[0.6840908, 0.89571583]\n",
            "Iteration 29, loss=[0.6835532, 0.9074959]\n",
            "Iteration 30, loss=[0.6829985, 0.91864055]\n",
            "Iteration 31, loss=[0.68242574, 0.92912257]\n",
            "Iteration 32, loss=[0.6818346, 0.9387904]\n",
            "Iteration 33, loss=[0.6812244, 0.9474923]\n",
            "Iteration 34, loss=[0.68059444, 0.95498824]\n",
            "Iteration 35, loss=[0.67994404, 0.96193177]\n",
            "Iteration 36, loss=[0.6792725, 0.96814406]\n",
            "Iteration 37, loss=[0.67857903, 0.97334176]\n",
            "Iteration 38, loss=[0.67786294, 0.977857]\n",
            "Iteration 39, loss=[0.67712337, 0.981466]\n",
            "Iteration 40, loss=[0.6763594, 0.98491967]\n",
            "Iteration 41, loss=[0.6755702, 0.9878697]\n",
            "Iteration 42, loss=[0.6747547, 0.9904387]\n",
            "Iteration 43, loss=[0.67391217, 0.9923524]\n",
            "Iteration 44, loss=[0.6730414, 0.9939574]\n",
            "Iteration 45, loss=[0.6721415, 0.9951941]\n",
            "Iteration 46, loss=[0.6712112, 0.99629897]\n",
            "Iteration 47, loss=[0.6702496, 0.9971728]\n",
            "Iteration 48, loss=[0.6692553, 0.9978155]\n",
            "Iteration 49, loss=[0.6682273, 0.99823797]\n",
            "Iteration 50, loss=[0.66716427, 0.9986514]\n",
            "Iteration 51, loss=[0.666065, 0.99895287]\n",
            "Iteration 52, loss=[0.664928, 0.9992237]\n",
            "Iteration 53, loss=[0.6637521, 0.99940604]\n",
            "Iteration 54, loss=[0.6625358, 0.99953604]\n",
            "Iteration 55, loss=[0.66127765, 0.9996299]\n",
            "Iteration 56, loss=[0.6599762, 0.99971116]\n",
            "Iteration 57, loss=[0.6586301, 0.9997671]\n",
            "Iteration 58, loss=[0.65723747, 0.99980503]\n",
            "Iteration 59, loss=[0.655797, 0.9998249]\n",
            "Iteration 60, loss=[0.65430707, 0.9998556]\n",
            "Iteration 61, loss=[0.65276587, 0.9998754]\n",
            "Iteration 62, loss=[0.6511718, 0.99988985]\n",
            "Iteration 63, loss=[0.6495233, 0.9998953]\n",
            "Iteration 64, loss=[0.64781857, 0.9998989]\n",
            "Iteration 65, loss=[0.6460557, 0.9999061]\n",
            "Iteration 66, loss=[0.6442332, 0.99991333]\n",
            "Iteration 67, loss=[0.64234924, 0.9999296]\n",
            "Iteration 68, loss=[0.640402, 0.9999332]\n",
            "Iteration 69, loss=[0.63838977, 0.99994224]\n",
            "Iteration 70, loss=[0.6363107, 0.99994403]\n",
            "Iteration 71, loss=[0.63416314, 0.99995124]\n",
            "Iteration 72, loss=[0.63194525, 0.99995667]\n",
            "Iteration 73, loss=[0.6296553, 0.99995667]\n",
            "Iteration 74, loss=[0.6272916, 0.9999603]\n",
            "Iteration 75, loss=[0.6248524, 0.9999603]\n",
            "Iteration 76, loss=[0.6223362, 0.9999639]\n",
            "Iteration 77, loss=[0.61974126, 0.9999675]\n",
            "Iteration 78, loss=[0.617066, 0.99997294]\n",
            "Iteration 79, loss=[0.6143091, 0.9999765]\n",
            "Iteration 80, loss=[0.61146885, 0.99997836]\n",
            "Iteration 81, loss=[0.60854405, 0.99998015]\n",
            "Iteration 82, loss=[0.6055335, 0.99998015]\n",
            "Iteration 83, loss=[0.60243577, 0.9999837]\n",
            "Iteration 84, loss=[0.5992501, 0.9999856]\n",
            "Iteration 85, loss=[0.5959752, 0.99998736]\n",
            "Iteration 86, loss=[0.59261036, 0.99998736]\n",
            "Iteration 87, loss=[0.58915484, 0.99998736]\n",
            "Iteration 88, loss=[0.585608, 0.99998736]\n",
            "Iteration 89, loss=[0.58196944, 0.99998736]\n",
            "Iteration 90, loss=[0.57823884, 0.99998736]\n",
            "Iteration 91, loss=[0.5744161, 0.9999928]\n",
            "Iteration 92, loss=[0.5705012, 0.999991]\n",
            "Iteration 93, loss=[0.56649446, 0.999991]\n",
            "Iteration 94, loss=[0.5623962, 0.999991]\n",
            "Iteration 95, loss=[0.5582073, 0.999991]\n",
            "Iteration 96, loss=[0.5539282, 0.999991]\n",
            "Iteration 97, loss=[0.5495601, 0.999991]\n",
            "Iteration 98, loss=[0.5451044, 0.999991]\n",
            "Iteration 99, loss=[0.5405624, 0.999991]\n",
            "Iteration 100, loss=[0.5359358, 0.999991]\n",
            "Iteration 101, loss=[0.5312267, 0.999991]\n",
            "Iteration 102, loss=[0.5264371, 0.999991]\n",
            "Iteration 103, loss=[0.52156943, 0.999991]\n",
            "Iteration 104, loss=[0.5166263, 0.999991]\n",
            "Iteration 105, loss=[0.5116106, 0.999991]\n",
            "Iteration 106, loss=[0.50652534, 0.999991]\n",
            "Iteration 107, loss=[0.5013738, 0.999991]\n",
            "Iteration 108, loss=[0.4961596, 0.999991]\n",
            "Iteration 109, loss=[0.49088633, 0.999991]\n",
            "Iteration 110, loss=[0.48555785, 0.999991]\n",
            "Iteration 111, loss=[0.48017827, 0.999991]\n",
            "Iteration 112, loss=[0.47475183, 0.999991]\n",
            "Iteration 113, loss=[0.4692828, 0.999991]\n",
            "Iteration 114, loss=[0.46377584, 0.999991]\n",
            "Iteration 115, loss=[0.45823556, 0.999991]\n",
            "Iteration 116, loss=[0.45266667, 0.999991]\n",
            "Iteration 117, loss=[0.44707394, 0.999991]\n",
            "Iteration 118, loss=[0.4414623, 0.999991]\n",
            "Iteration 119, loss=[0.4358366, 0.999991]\n",
            "Iteration 120, loss=[0.43020186, 0.999991]\n",
            "Iteration 121, loss=[0.42456293, 0.999991]\n",
            "Iteration 122, loss=[0.41892472, 0.999991]\n",
            "Iteration 123, loss=[0.41329208, 0.999991]\n",
            "Iteration 124, loss=[0.40766975, 0.999991]\n",
            "Iteration 125, loss=[0.40206248, 0.999991]\n",
            "Iteration 126, loss=[0.39647478, 0.999991]\n",
            "Iteration 127, loss=[0.39091104, 0.999991]\n",
            "Iteration 128, loss=[0.38537562, 0.999991]\n",
            "Iteration 129, loss=[0.37987265, 0.999991]\n",
            "Iteration 130, loss=[0.37440604, 0.999991]\n",
            "Iteration 131, loss=[0.3689795, 0.999991]\n",
            "Iteration 132, loss=[0.36359662, 0.999991]\n",
            "Iteration 133, loss=[0.35826072, 0.999991]\n",
            "Iteration 134, loss=[0.35297483, 0.999991]\n",
            "Iteration 135, loss=[0.34774193, 0.999991]\n",
            "Iteration 136, loss=[0.34256455, 0.999991]\n",
            "Iteration 137, loss=[0.3374451, 0.999991]\n",
            "Iteration 138, loss=[0.3323858, 0.999991]\n",
            "Iteration 139, loss=[0.32738853, 0.999991]\n",
            "Iteration 140, loss=[0.322455, 0.999991]\n",
            "Iteration 141, loss=[0.31758666, 0.999991]\n",
            "Iteration 142, loss=[0.31278473, 0.999991]\n",
            "Iteration 143, loss=[0.30805036, 0.999991]\n",
            "Iteration 144, loss=[0.30338424, 0.999991]\n",
            "Iteration 145, loss=[0.29878706, 0.999991]\n",
            "Iteration 146, loss=[0.29425925, 0.999991]\n",
            "Iteration 147, loss=[0.28980112, 0.999991]\n",
            "Iteration 148, loss=[0.28541276, 0.999991]\n",
            "Iteration 149, loss=[0.28109407, 0.999991]\n",
            "Iteration 150, loss=[0.27684492, 0.99998915]\n",
            "Iteration 151, loss=[0.27266496, 0.99998915]\n",
            "Iteration 152, loss=[0.26855382, 0.99998915]\n",
            "Iteration 153, loss=[0.26451087, 0.99998915]\n",
            "Iteration 154, loss=[0.26053557, 0.99998915]\n",
            "Iteration 155, loss=[0.2566271, 0.99998915]\n",
            "Iteration 156, loss=[0.25278473, 0.99998915]\n",
            "Iteration 157, loss=[0.24900766, 0.99998915]\n",
            "Iteration 158, loss=[0.24529487, 0.99998915]\n",
            "Iteration 159, loss=[0.24164547, 0.99998915]\n",
            "Iteration 160, loss=[0.23805843, 0.99998915]\n",
            "Iteration 161, loss=[0.23453274, 0.99998915]\n",
            "Iteration 162, loss=[0.23106739, 0.99998915]\n",
            "Iteration 163, loss=[0.2276613, 0.999991]\n",
            "Iteration 164, loss=[0.22431338, 0.999991]\n",
            "Iteration 165, loss=[0.22102255, 0.999991]\n",
            "Iteration 166, loss=[0.21778774, 0.999991]\n",
            "Iteration 167, loss=[0.2146079, 0.999991]\n",
            "Iteration 168, loss=[0.21148191, 0.999991]\n",
            "Iteration 169, loss=[0.20840873, 0.999991]\n",
            "Iteration 170, loss=[0.20538731, 0.999991]\n",
            "Iteration 171, loss=[0.20241661, 0.999991]\n",
            "Iteration 172, loss=[0.19949563, 0.999991]\n",
            "Iteration 173, loss=[0.19662334, 0.999991]\n",
            "Iteration 174, loss=[0.19379883, 0.999991]\n",
            "Iteration 175, loss=[0.19102108, 0.999991]\n",
            "Iteration 176, loss=[0.18828918, 0.999991]\n",
            "Iteration 177, loss=[0.1856022, 0.999991]\n",
            "Iteration 178, loss=[0.18295927, 0.999991]\n",
            "Iteration 179, loss=[0.18035953, 0.999991]\n",
            "Iteration 180, loss=[0.17780206, 0.999991]\n",
            "Iteration 181, loss=[0.17528616, 0.999991]\n",
            "Iteration 182, loss=[0.17281091, 0.999991]\n",
            "Iteration 183, loss=[0.17037559, 0.999991]\n",
            "Iteration 184, loss=[0.16797943, 0.999991]\n",
            "Iteration 185, loss=[0.16562168, 0.999991]\n",
            "Iteration 186, loss=[0.16330165, 0.999991]\n",
            "Iteration 187, loss=[0.16101862, 0.999991]\n",
            "Iteration 188, loss=[0.15877192, 0.999991]\n",
            "Iteration 189, loss=[0.15656088, 0.999991]\n",
            "Iteration 190, loss=[0.15438487, 0.999991]\n",
            "Iteration 191, loss=[0.15224327, 0.999991]\n",
            "Iteration 192, loss=[0.15013547, 0.999991]\n",
            "Iteration 193, loss=[0.14806089, 0.999991]\n",
            "Iteration 194, loss=[0.1460189, 0.999991]\n",
            "Iteration 195, loss=[0.144009, 0.999991]\n",
            "Iteration 196, loss=[0.1420306, 0.999991]\n",
            "Iteration 197, loss=[0.14008322, 0.999991]\n",
            "Iteration 198, loss=[0.1381663, 0.999991]\n",
            "Iteration 199, loss=[0.13627933, 0.999991]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}